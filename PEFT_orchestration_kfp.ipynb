{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cf11017-90e6-42ef-93cb-b6b676f514a3",
   "metadata": {},
   "source": [
    "# KFP Orchestration of Pipelin Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2fcd000-f502-47e5-b0db-809b9555f93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import dsl\n",
    "from kfp import compiler\n",
    "\n",
    "# Ignore FutureWarnings in kfp\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", \n",
    "                        category=FutureWarning, \n",
    "                        module='kfp.*')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294daf3a-41d6-4db2-84b0-bdab8f2acc0c",
   "metadata": {},
   "source": [
    "## Kubeflow Pipelines\n",
    "\n",
    "- Kubeflow pipelines consist of two key concepts: Components and pipelines.\n",
    "- Pipeline components are like self-contained sets of code that perform various steps in your ML workflow, such as, the first step could be preprocessing data, and second step could betraining a model.\n",
    "\n",
    "### Simple Pipeline Example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adb4746f-21e0-4bb2-9e95-75aae4a0d1e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Simple example: component 1\n",
    "@dsl.component\n",
    "def say_hello(name: str) -> str:\n",
    "    hello_text = f'Hello, {name}!'\n",
    "    \n",
    "    return hello_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cae0bd-91d1-453a-a307-fd06126449a7",
   "metadata": {},
   "source": [
    "- Since we \"wrapped\" this `say_hello` function in the decorator `@dsl.component`, the function will not actually return a string.\n",
    "- The function will return a `PipelineTask` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "112251c7-9c87-45cd-acff-de847a3c7208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<kfp.dsl.pipeline_task.PipelineTask object at 0x7ef6a4443730>\n"
     ]
    }
   ],
   "source": [
    "hello_task = say_hello(name=\"Erwin\")\n",
    "print(hello_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122a9205-934e-4b4e-b5c7-f832b6debadf",
   "metadata": {},
   "source": [
    "- The object that we'll use to pass the information in `hello_text` to other components in the pipeline is `PipelineTask.output`, which will be a built-in data type:\n",
    "> `['String', 'Integer', 'Float', 'Boolean', 'List', 'Dict']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "587b68a5-b6ec-4ebd-8a46-034684458d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{{channel:task=say-hello;name=Output;type=String;}}\n"
     ]
    }
   ],
   "source": [
    "print(hello_task.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701f75c7-e91a-4bed-beaa-e67dbb88953a",
   "metadata": {},
   "source": [
    "- Note when passing in values to the a `dsl.component` function, you have to specify the argument names (keyword arguments), and can't use positional arguments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea89276-bd5d-4ef3-b284-940678c18792",
   "metadata": {},
   "source": [
    "- The second component is dependent on the first component\n",
    "- Take the output of the first component and pass it to the second component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d0df10f-ba15-4ab5-ae30-79d128d03770",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Simple example: component 2\n",
    "@dsl.component\n",
    "def how_are_you(hello_text: str) -> str:\n",
    "    \n",
    "    how_are_you = f\"{hello_text}. How are you?\"\n",
    "    \n",
    "    return how_are_you"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b256fd9e-17e3-463c-b651-6c6d80dfdd46",
   "metadata": {},
   "source": [
    "- Notice that when we pass in the return value from the `say_hello` function, we want to pass in the PipelineTask.output object, and not the PipelineTask object itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a018a4c9-2436-401e-9bd4-8332c0b26fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<kfp.dsl.pipeline_task.PipelineTask object at 0x7ef6a42f7cd0>\n",
      "{{channel:task=how-are-you;name=Output;type=String;}}\n"
     ]
    }
   ],
   "source": [
    "how_task = how_are_you(hello_text=hello_task.output)\n",
    "print(how_task)\n",
    "print(how_task.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eabdac2-d9e5-4b69-a80a-6da0320e67ad",
   "metadata": {},
   "source": [
    "- Define the pipeline.\n",
    "- Notice how the input to `say_hello` is just `recipient`, since that is already a built-in data type (a String).\n",
    "- Recall that to get the value from a PipelineTask object, you'll use `PipelineTask.output` to pass in that value to another Pipeline Component function.\n",
    "- Notice that Pipeline function should return the PipelineTask.output as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75a4e200-a1a6-4262-a2af-565a535a5160",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Simple example: pipeline\n",
    "@dsl.pipeline\n",
    "def hello_pipeline(recipient: str) -> str:\n",
    "    \n",
    "    # notice, just recipient and not recipient.output\n",
    "    hello_task = say_hello(name=recipient)\n",
    "    \n",
    "    # notice .output\n",
    "    how_task = how_are_you(hello_text=hello_task.output)\n",
    "    \n",
    "    # notice .output\n",
    "    return how_task.output "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb31ba0-14a7-4be1-a9c7-8579cea026ed",
   "metadata": {},
   "source": [
    "- If you run this pipeline function, you'll see that the return value (`task.output` was a String) is again wrapped inside a PipelineTask object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57a7c498-f58d-430d-8f55-a453acdfa210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<kfp.dsl.pipeline_task.PipelineTask object at 0x7ef6a4443f70>\n"
     ]
    }
   ],
   "source": [
    "pipeline_output = hello_pipeline(recipient=\"Erwin\")\n",
    "print(pipeline_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359a70ee-191d-4f7f-ae90-8f1fc1dc40f7",
   "metadata": {},
   "source": [
    "- Note that if you tried to return a PipelineTask object instead of the PipelineTask.output, you'd get an error message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8f4c9a-ef44-45d1-8474-4aa0ed805180",
   "metadata": {},
   "source": [
    "##### Implement the pipeline\n",
    "\n",
    "- A pipeline is a set of components that you orchestrate.\n",
    "- It lets you define the order of execution and how data flows from one step to another.\n",
    "- Compile the pipeline into a yaml file, `test_pipeline.yaml`\n",
    "- You can look at the `test_pipeline.yaml` file in your workspace by going to `File --> Open...`. Or right here in the notebook (two cells below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9110c542-ab54-4573-af68-67961fb3f0be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(hello_pipeline, 'test_pipeline.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b8c308-6c5b-427f-809d-5ff13c51cd99",
   "metadata": {
    "tags": []
   },
   "source": [
    "- Define the arguments, the input that goes into the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "edbbf9bb-9b1c-48fa-b3bd-377bfb77982f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline_arguments = {\n",
    "    \"recipient\": \"World!\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42081e3d-8e8b-4672-a9e3-3d93695acb2c",
   "metadata": {},
   "source": [
    "- View the `test_pipeline.yaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4dcbbe99-c886-4e03-8a72-633b76d8b56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# PIPELINE DEFINITION\n",
      "# Name: hello-pipeline\n",
      "# Inputs:\n",
      "#    recipient: str\n",
      "# Outputs:\n",
      "#    Output: str\n",
      "components:\n",
      "  comp-how-are-you:\n",
      "    executorLabel: exec-how-are-you\n",
      "    inputDefinitions:\n"
     ]
    }
   ],
   "source": [
    "!head test_pipeline.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af790edc",
   "metadata": {},
   "source": [
    "- Load the Project ID and credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94d476bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import authenticate\n",
    "credentials, PROJECT_ID, STAGING_BUCKET = authenticate() \n",
    "REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15070570",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aiplatform\n",
    "\n",
    "aiplatform.init(project = PROJECT_ID,\n",
    "                location = REGION,\n",
    "                credentials = credentials,\n",
    "                staging_bucket=STAGING_BUCKET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7268560c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Bucket: llm-ft-bucket>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.cloud import storage\n",
    "storage_client = storage.Client(project=PROJECT_ID,credentials=credentials)\n",
    "list(storage_client.list_buckets())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19471f4a-7691-47bc-b963-9e830fae8c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/735245711206/locations/us-central1/pipelineJobs/hello-pipeline-20250613183644\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/735245711206/locations/us-central1/pipelineJobs/hello-pipeline-20250613183644')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/hello-pipeline-20250613183644?project=735245711206\n"
     ]
    }
   ],
   "source": [
    "# Set the pipeline root\n",
    "pipeline_root = f\"gs://{STAGING_BUCKET}/\"\n",
    "\n",
    "### import `PipelineJob` \n",
    "from google.cloud.aiplatform import PipelineJob\n",
    "\n",
    "job = PipelineJob(\n",
    "        ### path of the yaml file to execute\n",
    "        template_path=\"test_pipeline.yaml\",\n",
    "        ### name of the pipeline\n",
    "        display_name=f\"test_pipeline\",\n",
    "        ### pipeline arguments (inputs)\n",
    "        ### {\"recipient\": \"World!\"} for this example\n",
    "        parameter_values=pipeline_arguments,\n",
    "        ### region of execution\n",
    "        location=REGION,\n",
    "        ### root is where temporary files are being \n",
    "        ### stored by the execution engine\n",
    "        pipeline_root= pipeline_root\n",
    ")\n",
    "\n",
    "### submit for execution\n",
    "job.submit(service_account=\"llm-ft-sa@llmops-462607.iam.gserviceaccount.com\")\n",
    "### check to see the status of the job\n",
    "#job.state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e4334e-8c76-49e0-9b6b-309fe3cbc204",
   "metadata": {},
   "source": [
    "### Real-life Pipeline Example \n",
    "\n",
    "#### Automation and Orchestration of a Supervised Tuning Pipeline.\n",
    "\n",
    "- Reuse an existing Kubeflow Pipeline for Parameter-Efficient Fine-Tuning (PEFT) for a foundation model from Google, called [PaLM 2](https://ai.google/discover/palm2/). \n",
    "- Advantage of reusing a pipleline means you do not have to build it from scratch, you can only specify some of the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88178549-83df-4fa8-9554-a42e96558d19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### these are the same \n",
    "### jsonl files from the previous lab\n",
    "\n",
    "### time stamps have been removed so that \n",
    "### the files are consistent for all learners\n",
    "TRAINING_DATA_URI = \"./train_data_stack_overflow_python_qa.jsonl\" \n",
    "EVAUATION_DATA_URI = \"./eval_data_stack_overflow_python_qa.jsonl\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aafd1f73-9818-4e91-99e2-12bd5b02ca03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### path to the pipeline file to reuse\n",
    "### the file is provided in the workspace as well\n",
    "template_path = 'https://us-kfp.pkg.dev/ml-pipeline/google-cloud-registry/evaluation-llm-text-generation-pipeline/sha256:20d029b93c6182cc3d9e5a856d4b177f5aec56615df0590fca82bdb6a904cc49'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0de0a586-56df-4628-a6c7-b5b823b7168e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "date = datetime.datetime.now().strftime(\"%H:%d:%m:%Y\")\n",
    "\n",
    "MODEL_NAME = f\"llm-peft-{date}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86ab578-ac94-419d-813f-162251258e95",
   "metadata": {
    "tags": []
   },
   "source": [
    "- This example uses two PaLM model parameters:\n",
    "  - `TRAINING_STEPS`: Number of training steps to use when tuning the model. For extractive QA you can set it from 100-500. \n",
    "  - `EVALUATION_INTERVAL`: The interval determines how frequently a trained model is evaluated against the created *evaluation set* to assess its performance and identify issues. Default will be 20, which means after every 20 training steps, the model is evaluated on the evaluation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e44a637b-6f50-478d-8a58-8e0fc3b8b8c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRAINING_STEPS = 200\n",
    "EVALUATION_INTERVAL = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fc079e-5244-4421-9648-a9640b823388",
   "metadata": {
    "tags": []
   },
   "source": [
    "- Define the arguments, the input that goes into the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a8deddf9-abad-403d-9780-0a4b4ecbca41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline_arguments = {\n",
    "    #\"model_display_name\": MODEL_NAME,\n",
    "    \"location\": REGION,\n",
    "    \"model_name\": \"falcon-7b-instruct\",\n",
    "    \"project\": PROJECT_ID,\n",
    "    \"train_steps\": TRAINING_STEPS,\n",
    "    \"dataset_uri\": TRAINING_DATA_URI,\n",
    "    \"evaluation_interval\": EVALUATION_INTERVAL,\n",
    "    \"evaluation_data_uri\": EVAUATION_DATA_URI,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cddf05e9-40d2-474d-9d43-c9e644bf6053",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The pipeline parameter train_steps is not found in the pipeline job input definitions.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2063/956653001.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m job = PipelineJob(\n\u001b[0m\u001b[1;32m      2\u001b[0m         \u001b[0;31m### path of the yaml file to execute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mtemplate_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtemplate_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;31m### name of the pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mdisplay_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"peft_pipeline-{date}\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspaces/dl-ai-llmops/.venv/lib/python3.8/site-packages/google/cloud/aiplatform/pipeline_jobs.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, display_name, template_path, job_id, pipeline_root, parameter_values, input_artifacts, enable_caching, encryption_spec_key_name, labels, credentials, project, location, failure_policy)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_failure_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfailure_policy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m         \u001b[0mruntime_config_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0mruntime_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgca_pipeline_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPipelineJob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRuntimeConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspaces/dl-ai-llmops/.venv/lib/python3.8/site-packages/google/cloud/aiplatform/utils/pipeline_utils.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    184\u001b[0m         runtime_config = {\n\u001b[1;32m    185\u001b[0m             \u001b[0;34m\"gcsOutputDirectory\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pipeline_root\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m             parameter_values_key: {\n\u001b[0m\u001b[1;32m    187\u001b[0m                 \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_vertex_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameter_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspaces/dl-ai-llmops/.venv/lib/python3.8/site-packages/google/cloud/aiplatform/utils/pipeline_utils.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0;34m\"gcsOutputDirectory\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pipeline_root\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m             parameter_values_key: {\n\u001b[0;32m--> 187\u001b[0;31m                 \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_vertex_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameter_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspaces/dl-ai-llmops/.venv/lib/python3.8/site-packages/google/cloud/aiplatform/utils/pipeline_utils.py\u001b[0m in \u001b[0;36m_get_vertex_value\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameter_types\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    224\u001b[0m                 \u001b[0;34m\"The pipeline parameter {} is not found in the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                 \u001b[0;34m\"pipeline job input definitions.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The pipeline parameter train_steps is not found in the pipeline job input definitions."
     ]
    }
   ],
   "source": [
    "job = PipelineJob(\n",
    "        ### path of the yaml file to execute\n",
    "        template_path=template_path,\n",
    "        ### name of the pipeline\n",
    "        display_name=f\"peft_pipeline-{date}\",\n",
    "        ### pipeline arguments (inputs)\n",
    "        parameter_values=pipeline_arguments,\n",
    "        ### region of execution\n",
    "        location=REGION,\n",
    "        ### root is where temporary files are being \n",
    "        ### stored by the execution engine\n",
    "        pipeline_root=pipeline_root,\n",
    "        ### enable_caching=True will save the outputs \n",
    "        ### of components for re-use, and will only re-run those\n",
    "        ### components for which the code or data has changed.\n",
    "        enable_caching=True,\n",
    ")\n",
    "\n",
    "### submit for execution\n",
    "job.submit()\n",
    "\n",
    "### check to see the status of the job\n",
    "job.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7facc65c-f84b-40e8-876e-b2faa2a61135",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b61034-7243-4e03-8a46-9ef89a70bdcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-cpu.2-11:m113"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
